<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->

  <!-- Website verification -->
  
    <meta name="google-site-verification" content="">
  
  
    <meta name="msvalidate.01" content="">
  
  <!--
    Avoid warning on Google Chrome Error with Permissions-Policy header:
    Origin trial controlled feature not enabled: 'interest-cohort'.
    see https://stackoverflow.com/a/75119417
  -->
  <meta http-equiv="Permissions-Policy" content="interest-cohort=()">




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      publications | Chawin Sitawarin
    
  
</title>
<meta name="author" content="Chawin Sitawarin">
<meta name="description" content="publications by categories in reversed chronological order.&lt;br/&gt;* = equal contribution.">

  <meta name="keywords" content="adversarial examples, adversarial machine learning, privacy-preserving machine learning, robustness, LLM">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="/assets/img/favicon.ico?4aa7c6910923a196c8bb1a4cb3278095">

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="https://chawins.github.io/publications/">

<!-- Dark Mode -->
<script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Chawin</span>
            
            
            Sitawarin
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
          
            
          
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/llm-sp/">llm-sp
                    
                  </a>
                </li>
              
            
          
            
              
                
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/service/">services
                    
                  </a>
                </li>
              
            
          
            
          
            
          
          
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        

<div class="post">
  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications by categories in reversed chronological order.<br>* = equal contribution.</p>
  </header>

  <article>
    <!-- _pages/publications.md -->

<!-- Bibsearch Feature -->

<script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script>

<p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p>

<div class="publications">

<h2 class="bibliography">2025</h2>
<ol class="bibliography"><li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100">NAACL</abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/iris_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/iris_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="iris_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="huang_stronger_2025" class="col-sm-8">
    <!-- Title -->
    <div class="title">Stronger Universal and Transfer Attacks by Suppressing Refusals</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      David
            Huang, Avidan
            Shah, Alexandre
            Araujo, David
            Wagner, and <em>Chawin
            Sitawarin</em>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>,  Jan 2025
    </div>
    <div class="periodical">
      Also appeared in Neurips Safe Generative AI Workshop 2024.
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Making large language models (LLMs) safe for mass deployment is a complex and ongoing challenge. Efforts have focused on aligning models to human preferences (RLHF) in order to prevent malicious uses, essentially embedding a “safety feature” into the model’s parameters. The Greedy Coordinate Gradient (GCG) algorithm (Zou et al., 2023b) emerges as one of the most popular automated jailbreaks, an attack that circumvents this safety training. So far, it is believed that these optimization-based attacks are sample-specific as opposed to hand-crafted ones. To make the automated jailbreak universal and transferable, they require incorporating multiple samples and models into the objective function. Contrary to this belief, we find that the adversarial prompts discovered by such optimizers are inherently prompt-universal and transferable, even when optimized on a single model and a single harmful request. To further amplify this phenomenon, we introduce a new objective to these optimizers to explicitly deactivate the safety feature to create an even stronger universal and transferable attack. Without requiring a large number of queries or accessing output token probabilities, our transfer attack, optimized on Llama-3, achieves a 92% success rate against the state-of-the-art Circuit Breaker defense, compared to 2.5% by white-box GCG. Crucially, our method also attains state-of-the-art transfer rates on frontier models: GPT-3.5-Turbo (96%), GPT-4o (82%), and GPT-4o-mini (88%).</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">huang_stronger_2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Stronger Universal and Transfer Attacks by Suppressing Refusals}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Huang, David and Shah, Avidan and Araujo, Alexandre and Wagner, David and Sitawarin, Chawin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Mexico City, Mexico}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Also appeared in Neurips Safe Generative {{AI}} Workshop 2024.}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=kKOeZMIMlA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">Workshop</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/markmywords_thumbnail.jpg" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/markmywords_thumbnail.jpg" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="markmywords_thumbnail.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="piet_markmywords_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">MarkMyWords: Analyzing and Evaluating Language Model Watermarks</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Julien
            Piet, <em>Chawin
            Sitawarin</em>, Vivian
            Fang, Norman
            Mu, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Statistical Foundations of Llms and Foundation Models (NeurIPS 2024 Workshop)</em>,  Oct 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2312.00273" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/forum?id=lZJCtOWdoU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/MarkMyWords" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
        <a href="https://wagner-group.github.io/projects/markmywords/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. In this context, the ability to distinguish machine-generated text from human-authored content becomes important. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on text watermarking techniques - as opposed to image watermarks - and proposes a comprehensive benchmark for them under different tasks as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. Current watermarking techniques are good enough to be deployed: Kirchenbauer et al. can watermark Llama2-7B-chat with no perceivable loss in quality in under 100 tokens, and with good tamper-resistance to simple attacks, regardless of temperature. We argue that watermark indistinguishability is too strong a requirement: schemes that slightly modify logit distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. We publicly release our benchmark.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">piet_markmywords_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{MarkMyWords}}: {{Analyzing}} and Evaluating Language Model Watermarks}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Statistical Foundations of Llms and Foundation Models ({{NeurIPS}} 2024 Workshop)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Piet, Julien and Sitawarin, Chawin and Fang, Vivian and Mu, Norman and Wagner, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=lZJCtOWdoU}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Benchmark,LLM,Watermark}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://www.usenix.org/conference/usenixsecurity25" rel="external nofollow noopener" target="_blank">USENIX Security</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/struq_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/struq_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="struq_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="chen_struq_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">StruQ: Defending against Prompt Injection with Structured Queries</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Sizhe
            Chen, Julien
            Piet, <em>Chawin
            Sitawarin</em>, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 34th USENIX Security Symposium (USENIX Security 25)</em>,  Feb 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2402.06363" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
      
      
      
      
        <a href="https://github.com/sizhe-chen/struq" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language understanding capabilities. However, as LLMs have improved, so have the attacks against them. Prompt injection attacks are an important threat: they trick the model to deviate from the original application’s instructions and instead follow user directives. These attacks rely on the LLM’s ability to follow instructions and inability to separate the prompts and user data. We introduce structured queries, a general approach to tackle this problem. Structured queries separate prompts and data into two channels. We implement a system that supports structured queries. This system is made of (1) a secure front-end that formats a prompt and user data into a special format, and (2) a specially trained LLM that can produce high-quality outputs from these inputs. The LLM is trained using a novel fine-tuning strategy: we convert a base (non-instruction-tuned) LLM to a structured instruction-tuned model that will only follow instructions in the prompt portion of a query. To do so, we augment standard instruction tuning datasets with examples that also include instructions in the data portion of the query, and fine-tune the model to ignore these. Our system significantly improves resistance to prompt injection attacks, with little or no impact on utility. Our code is released at https://github.com/Sizhe-Chen/PromptInjectionDefense.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chen_struq_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{StruQ}}: Defending against Prompt Injection with Structured Queries}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{34th {{USENIX}} Security Symposium ({{USENIX}} Security 25)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2402.06363}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/oodrobust_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/oodrobust_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="oodrobust_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="li_oodrobustbench_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">OODRobustBench: A Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Lin
            Li, Yifei
            Wang, <em>Chawin
            Sitawarin</em>, and Michael W.
            Spratling
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 41st International Conference on Machine Learning</em>,  Jul 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2310.12793" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/forum?id=RnYd44LR2v" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/oodrobustbench/oodrobustbench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This omission is concerning as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness in a positive linear way. The latter enables the prediction of OOD robustness from ID robustness. We then predict and verify that existing methods are unlikely to achieve high OOD robustness. Novel methods are therefore required to achieve OOD robustness beyond our prediction. To facilitate the development of these methods, we investigate a wide range of techniques and identify several promising directions. Code and models are available at: https://github.com/OODRobustBench/OODRobustBench.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li_oodrobustbench_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{OODRobustBench}}: A Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 41st International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Lin and Wang, Yifei and Sitawarin, Chawin and Spratling, Michael W.}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{235}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{28830--28869}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v235/li24bp.html}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://conf.researchr.org/home/icse-2025" rel="external nofollow noopener" target="_blank">ICSE</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/primevul_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/primevul_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="primevul_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ding_vulnerability_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">Vulnerability Detection with Code Language Models: How Far Are We?</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Yangruibo
            Ding, Yanjun
            Fu, Omniyyah
            Ibrahim, <em>Chawin
            Sitawarin</em>, Xinyun
            Chen, Basel
            Alomair, David
            Wagner, Baishakhi
            Ray, and Yizheng
            Chen
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the IEEE/ACM 47th International Conference on Software Engineering</em>,  Mar 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2403.18624" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
      
      
      
      
        <a href="https://github.com/dlvuldet/primevul" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection. To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs’ performance in real-world conditions. Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ding_vulnerability_2024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ding, Yangruibo and Fu, Yanjun and Ibrahim, Omniyyah and Sitawarin, Chawin and Chen, Xinyun and Alomair, Basel and Wagner, David and Ray, Baishakhi and Chen, Yizheng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Vulnerability Detection with Code Language Models: How Far Are We?}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2403.18624}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/ACM 47th International Conference on Software Engineering}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{ICSE '25}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://esorics2024.org/" rel="external nofollow noopener" target="_blank">ESORICS</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jatmo_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/jatmo_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="jatmo_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="piet_jatmo_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">Jatmo: Prompt Injection Defense by Task-Specific Finetuning</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Julien
            Piet<sup>*</sup>, Maha
            Alrashed<sup>*</sup>, <em>Chawin
            Sitawarin</em>, Sizhe
            Chen, Zeming
            Wei, Elizabeth
            Sun, Basel
            Alomair, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Computer Security – ESORICS 2024</em>,  Mar 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2312.17673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://dl.acm.org/doi/abs/10.1007/978-3-031-70879-4_6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/prompt-injection-defense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large Language Models (LLMs) are attracting significant research attention due to their instruction-following abilities, allowing users and developers to leverage LLMs for a variety of tasks. However, LLMs are vulnerable to prompt-injection attacks: a class of attacks that hijack the model’s instruction-following abilities, changing responses to prompts to undesired, possibly malicious ones. In this work, we introduce Jatmo, a method for generating task-specific models resilient to prompt-injection attacks. Jatmo leverages the fact that LLMs can only follow instructions once they have undergone instruction tuning. It harnesses a teacher instruction-tuned model to generate a task-specific dataset, which is then used to fine-tune a base model (i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a dataset of inputs for the task: it uses the teacher model to generate outputs. For situations with no pre-existing datasets, Jatmo can use a single example, or in some cases none at all, to produce a fully synthetic dataset. Our experiments on six tasks show that Jatmo models provide the same quality of outputs on their specific task as standard LLMs, while being resilient to prompt injections. The best attacks succeeded in less than 0.5% of cases against our models, versus over 90% success rate against GPT-3.5-Turbo. We release Jatmo at https://github.com/wagner-group/prompt-injection-defense.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">piet_jatmo_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Jatmo: Prompt Injection Defense by Task-Specific Finetuning}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Jatmo}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Piet{${}$}, Julien and Alrashed{${}$}, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2312.17673}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer {{Security}} -- {{ESORICS}} 2024}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-01-03}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#b31b1b">
            
              <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Preprint</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pal_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/pal_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="pal_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_pal_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">PAL: Proxy-Guided Black-Box Attack on Large Language Models</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Norman
            Mu, David
            Wagner, and Alexandre
            Araujo
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Under submission</em>,  Feb 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2402.09674" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
      
      
      
      
        <a href="https://github.com/chawins/pal" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Large Language Models (LLMs) have surged in popularity in recent months, but they have demonstrated concerning capabilities to generate harmful content when manipulated. While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs (PAL), the first optimization-based attack on LLMs in a black-box query-only setting. In particular, it relies on a surrogate model to guide the optimization and a sophisticated loss designed for real-world LLM APIs. Our attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on Llama-2-7B, compared to 4% for the current state of the art. We also propose GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple baseline for query-based attacks. We believe the techniques proposed in this work will enable more comprehensive safety testing of LLMs and, in the long term, the development of better security guardrails. The code can be found at https://github.com/chawins/pal.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sitawarin_pal_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{PAL}}: Proxy-Guided Black-Box Attack on Large Language Models}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{PAL}}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Mu, Norman and Wagner, David and Araujo, Alexandre}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{arXiv:2402.09674}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.09674}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{arXiv}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://arxiv.org/abs/2402.09674}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2024-02-16}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution 4.0 International License (CC-BY)}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under submission}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pubdef_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/pubdef_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="pubdef_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_defending_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">PubDef: Defending against Transfer Attacks from Public Models</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Jaewon
            Chang<sup>*</sup>, David
            Huang<sup>*</sup>, Wesson
            Altoyan, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Twelfth International Conference on Learning Representations</em>,  Jan 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2310.17645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/forum?id=Tvwf4Vsi5F" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/pubdef" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
        
          <a href="/assets/pdf/pubdef_iclr2024_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
        <a href="https://wagner-group.github.io/projects/pubdef/index.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear way, under many distribution shifts. The latter enables the prediction of OOD robustness from ID robustness. Based on this, we are able to predict the upper limit of OOD robustness for existing robust training schemes. The results suggest that achieving OOD robustness requires designing novel methods beyond the conventional ones. Last, we discover that extra data, data augmentation, advanced model architectures and particular regularization approaches can improve OOD robustness. Noticeably, the discovered training schemes, compared to the baseline, exhibit dramatically higher robustness under threat shift while keeping high ID robustness, demonstrating new promising solutions for robustness against both multi-attack and unforeseen attacks.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_defending_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PubDef: Defending against Transfer Attacks from Public Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Chang{${}$}, Jaewon and Huang{${}$}, David and Altoyan, Wesson and Wagner, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{CC0 1.0 Universal Public Domain Dedication}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=Tvwf4Vsi5F}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,notion}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spder_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/spder_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="spder_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="shah_spder_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">SPDER: Semiperiodic Damping-Enabled Object Representation</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Kathan
            Shah, and <em>Chawin
            Sitawarin</em>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In The Twelfth International Conference on Learning Representations</em>,  Jan 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2306.15242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/forum?id=92btneN9Wm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/katop1234/SPDER/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional implicit neural representation networks. Our proposed architecture, SPDER, is a simple MLP that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. Our results indicate that SPDERs speed up training by 10x and converge to losses 1,500-50,000x lower than that of the state-of-the-art for image representation. SPDER is also state-of-the-art in audio representation. The superior representation capability allows SPDER to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation. We provide intuition as to why SPDER significantly improves fitting compared to that of other INR methods while requiring no hyperparameter tuning or preprocessing.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">shah_spder_2024</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{SPDER}}: Semiperiodic Damping-Enabled Object Representation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shah, Kathan and Sitawarin, Chawin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=92btneN9Wm}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography">
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://iccv2023.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/reap_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/reap_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="reap_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="hingun_reap_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">REAP: A Large-Scale Realistic Adversarial Patch Benchmark</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Nabeel
            Hingun<sup>*</sup>, <em>Chawin
            Sitawarin<sup>*</sup></em>, Jerry
            Li, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>,  Oct 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2212.05680" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/reap-benchmark" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/reap_poster_iccv2023.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a sticker with a particularly crafted pattern that makes the model incorrectly predict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) Benchmark, a digital benchmark that allows the user to evaluate patch attacks on real images, and under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with a pair of geometric and lighting transformations, which can be used to apply a digitally generated patch realistically onto the sign, while matching real-world conditions. Using our benchmark, we perform the first large-scale assessments of adversarial patch attacks under realistic conditions. Our experiments suggest that adversarial patch attacks may present a smaller threat than previously believed and that the success rate of an attack on simpler digital simulations is not predictive of its actual effectiveness in practice.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hingun_reap_2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hingun{${}$}, Nabeel and Sitawarin{${}$}, Chawin and Li, Jerry and Wagner, David}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{REAP: A Large-Scale Realistic Adversarial Patch Benchmark}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4640-4651}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/prep_attack_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/prep_attack_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="prep_attack_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_preprocessors_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Florian
            Tramèr, and Nicholas
            Carlini
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 40th International Conference on Machine Learning</em>,  Jul 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2210.03297" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://proceedings.mlr.press/v202/sitawarin23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/google-research/preprocessor-aware-black-box-attack" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/preprocessor_attack_poster_icml2023.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/preprocessor_attack_slides_icml2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Decision-based attacks construct adversarial examples against a machine learning (ML) model by making only hard-label queries. These attacks have mainly been applied directly to standalone neural networks. However, in practice, ML models are just one component of a larger learning system. We find that by adding a single preprocessor in front of a classifier, state-of-the-art query-based attacks are up to seven\texttimes less effective at attacking a prediction pipeline than at attacking the model alone. We explain this discrepancy by the fact that most preprocessors introduce some notion of invariance to the input space. Hence, attacks that are unaware of this invariance inevitably waste a large number of queries to re-discover or overcome it. We, therefore, develop techniques to (i) reverse-engineer the preprocessor and then (ii) use this extracted information to attack the end-to-end system. Our preprocessors extraction method requires only a few hundred queries, and our preprocessor-aware attacks recover the same efficacy as when attacking the model alone. The code can be found at https://github.com/google-research/preprocessor-aware-black-box-attack.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_preprocessors_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Preprocessors Matter! {{Realistic}} Decision-Based Attacks on Machine Learning Systems}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Tram{\`e}r, Florian and Carlini, Nicholas}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{32008--32032}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{PMLR}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v202/sitawarin23a.html}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank">ICLR</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/part_model_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/part_model_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="part_model_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_partbased_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">Part-Based Models Improve Adversarial Robustness</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Kornrapat
            Pongmala, Yizheng
            Chen, Nicholas
            Carlini, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In International Conference on Learning Representations</em>,  May 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2209.09117" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/pdf?id=bAMTaeqluh4" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/chawins/adv-part-model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
        
          <a href="/assets/pdf/part_model_slides_iclr2023.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline’s, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_partbased_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Part-Based Models Improve Adversarial Robustness}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Pongmala, Kornrapat and Chen, Yizheng and Carlini, Nicholas and Wagner, David}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=bAMTaeqluh4}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://www.ndss-symposium.org/ndss-program/vehiclesec-2023/" rel="external nofollow noopener" target="_blank">VehicleSec</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/patch_cert_survey_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/patch_cert_survey_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="patch_cert_survey_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="xiang_short_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">Short: Certifiably Robust Perception against Adversarial Patch Attacks: A Survey</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Chong
            Xiang, <em>Chawin
            Sitawarin</em>, Tong
            Wu, and Prateek
            Mittal
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 1st Symposium on Vehicle Security and Privacy (VehicleSec)</em>,  Mar 2023
    </div>
    <div class="periodical">
      Co-located with NDSS 2023
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="award btn btn-sm z-depth-0" role="button">Best Paper Runner-Up</a>
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://www.ndss-symposium.org/wp-content/uploads/2023/02/vehiclesec2023-23020-paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/inspire-group/patch-defense-leaderboard" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="https://drive.google.com/file/d/1vPfkU0tPVQ-aBC_mEa_LsYZkmjPqyiEP/view" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a>
        
      
      
        
          <a href="https://docs.google.com/presentation/d/1PEQrz8lD0Bzl6QCtBUzk2xULgsRgGRTm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Best Short/WIP Paper Award Runner-Up</p>

      </div>
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The physical-world adversarial patch attack poses a security threat to AI perception models in autonomous vehicles. To mitigate this threat, researchers have designed defenses with certifiable robustness. In this paper, we survey existing certifiably robust defenses and highlight core robustness techniques that are applicable to a variety of perception tasks, including classification, detection, and segmentation. We emphasize the unsolved problems in this space to guide future research, and call for attention and efforts from both academia and industry to robustify perception models in autonomous vehicles.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xiang_short_2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Short: Certifiably Robust Perception against Adversarial Patch Attacks: A Survey}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{1st Symposium on {{Vehicle Security}} and {{Privacy}} ({{VehicleSec}})}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xiang, Chong and Sitawarin, Chawin and Wu, Tong and Mittal, Prateek}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">langid</span> <span class="p">=</span> <span class="s">{english}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Co-located with {{NDSS}} 2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography"><li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/demysify_icml2021_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/demysify_icml2021_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="demysify_icml2021_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_demystifying_2022" class="col-sm-8">
    <!-- Title -->
    <div class="title">Demystifying the Adversarial Robustness of Random Transformation Defenses</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Zachary
            Golan-Strieb, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 39th International Conference on Machine Learning</em>,  Mar 2022
    </div>
    <div class="periodical">
      Also appeared in AAAI-2022 Workshop on Adversarial Machine Learning and Beyond
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="award btn btn-sm z-depth-0" role="button">Best Workshop Paper</a>
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2207.03574" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://proceedings.mlr.press/v162/sitawarin22a/sitawarin22a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/demystify-random-transform" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
        
          <a href="/assets/pdf/demysify_random_transform_icml22_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
        
      
      
        
          <a href="/assets/pdf/demysify_random_transform_icml22_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    
      <!-- Hidden Award block -->
      <div class="award hidden d-print-inline">
        <p></p>
<p>Best Paper Award from AAAI-2022 Workshop on Adversarial Machine Learning and Beyond</p>

      </div>
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Neural networks’ lack of robustness against attacks raises concerns in security-sensitive settings
                 such as autonomous vehicles. While many countermeasures may look promising, only a few withstand rigorous evaluation.
                 Defenses using random transformations (RT) have shown impressive results, particularly BaRT (Raff et al., 2019) on ImageNet.
                 However, this type of defense has not been rigorously evaluated, leaving its robustness properties poorly understood.
                 Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable.
                 First, we show that the BPDA attack (Athalye et al., 2018a) used in BaRT’s evaluation is ineffective and likely over-estimates its robustness.
                 We then attempt to construct the strongest possible RT defense through the informed selection of transformations and
                 Bayesian optimization for tuning their parameters. Furthermore, we create the strongest possible attack to evaluate our RT defense.
                 Our new attack vastly outperforms the baseline, reducing the accuracy by 83% compared to the 19% reduction by the commonly
                 used EoT attack (4.3x improvement). Our result indicates that the RT defense on Imagenette dataset (ten-class subset of ImageNet)
                 is not robust against adversarial examples. Extending the study further, we use our new attack to adversarially train RT defense
                 (called AdvRT), resulting in a large robustness gain.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_demystifying_2022</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Golan-Strieb, Zachary and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Demystifying the Adversarial Robustness of Random Transformation Defenses}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v162/sitawarin22a/sitawarin22a.pdf}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Also appeared in AAAI-2022 Workshop on Adversarial Machine Learning and Beyond}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography">
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/knn_neurips2021_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/knn_neurips2021_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="knn_neurips2021_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_adversarial_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Evgenios M
            Kornaropoulos, Dawn
            Song, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Advances in Neural Information Processing Systems</em>,  Mar 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2011.09719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://openreview.net/forum?id=2j3B_YkC8r" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/geoadex" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Adversarial examples are a widely studied phenomenon in machine learning models. While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of k-nearest neighbor classification, i.e., finding a minimum-norm adversarial example. Diverging from previous proposals, we propose the first geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby higher-order Voronoi cells until we find a cell that classifies differently from the input point. To scale the algorithm to a large k, we introduce approximation steps that find perturbation with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_adversarial_2021</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Kornaropoulos, Evgenios M and Song, Dawn and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Curran Associates, Inc.}}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Adversarial Examples for k-Nearest Neighbor Classifiers Based on Higher-Order Voronoi Diagrams}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">Workshop</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tradeoff_urdl2021_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/tradeoff_urdl2021_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="tradeoff_urdl2021_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_improving_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">Improving the Accuracy-Robustness Trade-off for Dual-Domain Adversarial Training</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Arvind P
            Sridhar, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Workshop on Uncertainty and Robustness in Deep Learning</em>,  Jul 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://chawins.github.io/assets/pdf/UDL2021-paper-048.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/wagner-group/dual-domain-at" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>While Adversarial Training remains the standard in improving robustness to adversarial attack, it often sacrifices accuracy on natural (clean) samples to a significant extent. Dual-domain training, optimizing on both clean and adversarial objectives, can help realize a better trade-off between clean accuracy and robustness. In this paper, we develop methods to improve dual-domain training for large adversarial perturbations and complex datasets. We first demonstrate that existing methods suffer from poor performance in this setting, due to a poor training procedure and overfitting to a particular attack. Then, we develop novel methods to address these issues. First, we show that adding KLD regularization to the dual training objective mitigates this overfitting and achieves a better trade-off, on CIFAR-10 and a 10-class subset of ImageNet. Then, inspired by domain adaptation, we develop a new normalization technique, Dual Batch Normalization, to further improve accuracy. Combining these two strategies, our model sets a new state of the art in trade-off performance for dual-domain adversarial training.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_improving_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Improving the Accuracy-Robustness Trade-off for Dual-Domain Adversarial Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Sridhar, Arvind P and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on {{Uncertainty}} and {{Robustness}} in {{Deep Learning}}}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://aisec.cc/" rel="external nofollow noopener" target="_blank">AISec</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sat_aisec2019_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/sat_aisec2019_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="sat_aisec2019_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_sat_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Supriyo
            Chakraborty, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security</em>, Virtual Event, Republic of Korea,  Jul 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2003.09347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://dl.acm.org/doi/abs/10.1145/3474369.3486878" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
        
          <a href="/assets/pdf/aisec_2021.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
        
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Adversarial training (AT) has become a popular choice for training robust networks.
                 However, it tends to sacrifice clean accuracy heavily in favor of robustness and suffers
                 from a large generalization error. To address these concerns, we propose Smooth Adversarial
                 Training (SAT), guided by our analysis on the eigenspectrum of the loss Hessian. We
                 find that curriculum learning, a scheme that emphasizes on starting "easy” and gradually
                 ramping up on the "difficulty” of training, smooths the adversarial loss landscape
                 for a suitably chosen difficulty metric. We present a general formulation for curriculum
                 learning in the adversarial setting and propose two difficulty metrics based on the
                 maximal Hessian eigenvalue (H-SAT) and the softmax probability (P-SA). We demonstrate
                 that SAT stabilizes network training even for a large perturbation norm and allows
                 the network to operate at a better clean accuracy versus robustness trade-off curve
                 compared to AT. This leads to a significant improvement in both clean accuracy and
                 robustness compared to AT, TRADES, and other baselines. To highlight a few results,
                 our best model improves normal and robust accuracy by 6% and 1% on CIFAR-100 compared
                 to AT, respectively. On Imagenette, a ten-class subset of ImageNet, our model outperforms
                 AT by 23% and 3% on normal and robust accuracy respectively.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_sat_2021</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Chakraborty, Supriyo and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3474369.3486878}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450386579}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{adversarial examples, adversarial machine learning, curriculum learning}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, Republic of Korea}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{25–36}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{AISec '21}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3474369.3486878}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">Workshop</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mitigating_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/mitigating_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="mitigating_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sridhar_mitigating_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">Mitigating Adversarial Training Instability with Batch Normalization</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Arvind P
            Sridhar, <em>Chawin
            Sitawarin</em>, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Security and Safety in Machine Learning Systems Workshop</em>,  May 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/43.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The adversarial training paradigm has become the standard in training deep neural networks for robustness. Yet, it remains unstable, with the mechanisms driving this instability poorly understood. In this study, we discover that this instability is primarily driven by a non-smooth optimization landscape and an internal covariate shift phenomenon, and show that Batch Normalization (BN) can effectively mitigate both these issues. Further, we demonstrate that BN universally improves clean and robust performance across various defenses, datasets, and model types, with greater improvement on more difficult tasks. Finally, we confirm BN’s heterogeneous distribution issue with mixed-batch training and propose a solution.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sridhar_mitigating_2021</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sridhar, Arvind P and Sitawarin, Chawin and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Security and {{Safety}} in {{Machine Learning Systems Workshop}}}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Mitigating Adversarial Training Instability with Batch Normalization}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2020</h2>
<ol class="bibliography"><li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://dls2023.ieee-security.org/" rel="external nofollow noopener" target="_blank">DLS</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/minnorm_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/minnorm_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="minnorm_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_minimumnorm_2020" class="col-sm-8">
    <!-- Title -->
    <div class="title">Minimum-Norm Adversarial Examples on KNN and KNN Based Models</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2020 IEEE Security and Privacy Workshops (SPW)</em>,  May 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/2003.06559" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://ieeexplore.ieee.org/document/9283888" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/chawins/knn-defense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We study the robustness against adversarial examples of kNN classifiers and classifiers that combine kNN with neural networks. The main difficulty lies in the fact that finding an optimal attack on kNN is intractable for typical datasets. In this work, we propose a gradient-based attack on kNN and kNN-based defenses, inspired by the previous work by Sitawarin &amp; Wagner [1]. We demonstrate that our attack outperforms their method on all of the models we tested with only a minimal increase in the computation time. The attack also beats the state-of-the-art attack [2] on kNN when k &gt; 1 using less than 1% of its running time. We hope that this attack can be used as a new baseline for evaluating the robustness of kNN and its variants.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_minimumnorm_2020</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{{Los Alamitos, CA, USA}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 {{IEEE}} Security and Privacy Workshops ({{SPW}})}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/SPW50608.2020.00023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{computational modeling,conferences,data privacy,neural networks,robustness,security}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{34--40}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE Computer Society}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Minimum-Norm Adversarial Examples on {{KNN}} and {{KNN}} Based Models}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography">
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://aisec.cc/" rel="external nofollow noopener" target="_blank">AISec</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ood_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/ood_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="ood_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sehwag_analyzing_2019" class="col-sm-8">
    <!-- Title -->
    <div class="title">Analyzing the Robustness of Open-World Machine Learning</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Vikash
            Sehwag, Arjun Nitin
            Bhagoji, Liwei
            Song, <em>Chawin
            Sitawarin</em>, Daniel
            Cullina, Mung
            Chiang, and Prateek
            Mittal
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security</em>,  May 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://dl.acm.org/doi/pdf/10.1145/3338501.3357372" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/inspire-group/OOD-Attacks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing øodAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that øodAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sehwag_analyzing_2019</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{{New York, NY, USA}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sehwag, Vikash and Bhagoji, Arjun Nitin and Song, Liwei and Sitawarin, Chawin and Cullina, Daniel and Chiang, Mung and Mittal, Prateek}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 12th {{ACM}} Workshop on Artificial Intelligence and Security}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3338501.3357372}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-4503-6833-9}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{adversarial example,deep learning,open world recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105--116}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Association for Computing Machinery}}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{{{AISec}}'19}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Analyzing the Robustness of Open-World Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#b31b1b">
            
              <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Preprint</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/defending_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/defending_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="defending_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_defending_2019" class="col-sm-8">
    <!-- Title -->
    <div class="title">Defending against Adversarial Examples with K-Nearest Neighbor</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv:1906.09525 [cs]</em>,  Jun 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/1906.09525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://arxiv.org/abs/1906.09525" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/chawins/knn-defense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>(We took the paper down from arXiv because the defense is broken by our new attack. The paper is still available [here](https://drive.google.com/file/d/1_3SjKi92mfCRAg99EXEJXpOpGCCw2OXN/view?usp=sharing)) Robustness is an increasingly important property of machine learning models as they become more and more prevalent. We propose a defense against adversarial examples based on a k-nearest neighbor (kNN) on the intermediate activation of neural networks. Our scheme surpasses state-of-the-art defenses on MNIST and CIFAR-10 against l2-perturbation by a significant margin. With our models, the mean perturbation norm required to fool our MNIST model is 3.07 and 2.30 on CIFAR-10. Additionally, we propose a simple certifiable lower bound on the l2-norm of the adversarial perturbation using a more specific version of our scheme, a 1-NN on representations learned by a Lipschitz network. Our model provides a nontrivial average lower bound of the perturbation norm, comparable to other schemes on MNIST with similar clean accuracy.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sitawarin_defending_2019</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Wagner, David}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1906.09525}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1906.09525 [cs]}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Defending against Adversarial Examples with K-Nearest Neighbor}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://dls2023.ieee-security.org/" rel="external nofollow noopener" target="_blank">DLS</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/deepknn_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/deepknn_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="deepknn_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_robustness_2019" class="col-sm-8">
    <!-- Title -->
    <div class="title">On the Robustness of Deep K-Nearest Neighbors</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, and David
            Wagner
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2019 IEEE Security and Privacy Workshops (SPW)</em>,  May 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/1903.08333" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://arxiv.org/abs/1903.08333" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/chawins/knn-defense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Despite a large amount of attention on adversarial examples, very few works have demonstrated an effective defense against this threat. We examine Deep k-Nearest Neighbor (DkNN), a proposed defense that combines k-Nearest Neighbor (kNN) and deep learning to improve the model’s robustness to adversarial examples. It is challenging to evaluate the robustness of this scheme due to a lack of efficient algorithm for attacking kNN classifiers with large k and high-dimensional data. We propose a heuristic attack that allows us to use gradient descent to find adversarial examples for kNN classifiers, and then apply it to attack the DkNN defense as well. Results suggest that our attack is moderately stronger than any naive attack on kNN and significantly outperforms other attacks on DkNN.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sitawarin_robustness_2019</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{{Los Alamitos, CA, USA}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Wagner, David}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 {{IEEE}} Security and Privacy Workshops ({{SPW}})}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/SPW.2019.00014}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{adaptation models,deep learning,neural networks,optimization,perturbation methods,robustness,training}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--7}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{IEEE Computer Society}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Robustness of Deep K-Nearest Neighbors}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography">
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://www.itsoc.org/event/58th-annual-conference-information-sciences-and-systems-ciss" rel="external nofollow noopener" target="_blank">CISS</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="bhagoji_enhancing_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">Enhancing Robustness of Machine Learning Systems via Data Transformations</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Arjun Nitin
            Bhagoji, Daniel
            Cullina, <em>Chawin
            Sitawarin</em>, and Prateek
            Mittal
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 52nd Annual Conference on Information Sciences and Systems (CISS)</em>,  May 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://arxiv.org/abs/1704.02654" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/inspire-group/ml_defense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis and data "anti-whitening" to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">bhagoji_enhancing_2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bhagoji, Arjun Nitin and Cullina, Daniel and Sitawarin, Chawin and Mittal, Prateek}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{52nd Annual Conference on Information Sciences and Systems ({{CISS}})}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CISS.2018.8362326}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--5}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Enhancing Robustness of Machine Learning Systems via Data Transformations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://www.sigsac.org/ccs/CCS2024/" rel="external nofollow noopener" target="_blank">CCS</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sehwag_not_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">Not All Pixels Are Born Equal: An Analysis of Evasion Attacks under Locality Constraints</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Vikash
            Sehwag, <em>Chawin
            Sitawarin</em>, Arjun Nitin
            Bhagoji, Arsalan
            Mosenia, Mung
            Chiang, and Prateek
            Mittal
      
        <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="4 citations (Semantic Scholar/DOI) [2021-06-11] 0 citations (Crossref) [2021-06-11] 00000">
        </i>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security</em>,  Oct 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://dl.acm.org/citation.cfm?id=3278515" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Deep neural networks (DNNs) have enabled success in learning tasks such as image classification, semantic image segmentation and steering angle prediction which can be key components of the computer vision pipeline of safety-critical systems such as autonomous vehicles. However, previous work has demonstrated the feasibility of using physical adversarial examples to attack image classification systems.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sehwag_not_2018</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{{Toronto Canada}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sehwag, Vikash and Sitawarin, Chawin and Bhagoji, Arjun Nitin and Mosenia, Arsalan and Chiang, Mung and Mittal, Prateek}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2018 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}}}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3243734.3278515}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-4503-5693-0}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2285--2287}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{ACM}}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Not All Pixels Are Born Equal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Not All Pixels Are Born Equal: An Analysis of Evasion Attacks under Locality Constraints}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#b31b1b">
            
              <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Preprint</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_darts_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">DARTS: Deceiving Autonomous Cars with Toxic Signs</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Arjun Nitin
            Bhagoji, Arsalan
            Mosenia, Mung
            Chiang, and Prateek
            Mittal
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv:1802.06430 [cs]</em>,  May 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://arxiv.org/abs/1802.06430" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/inspire-group/advml-traffic-sign" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Sign recognition is an integral part of autonomous cars. Any misclassification of traffic signs can potentially lead to a multitude of disastrous consequences, ranging from a life-threatening accident to even a large-scale interruption of transportation services relying on autonomous cars. In this paper, we propose and examine security attacks against sign recognition systems for Deceiving Autonomous caRs with Toxic Signs (we call the proposed attacks DARTS). In particular, we introduce two novel methods to create these toxic signs. First, we propose Out-of-Distribution attacks, which expand the scope of adversarial examples by enabling the adversary to generate these starting from an arbitrary point in the image space compared to prior attacks which are restricted to existing training/test data (In-Distribution). Second, we present the Lenticular Printing attack, which relies on an optical phenomenon to deceive the traffic sign recognition system. We extensively evaluate the effectiveness of the proposed attacks in both virtual and real-world settings and consider both white-box and black-box threat models. Our results demonstrate that the proposed attacks are successful under both settings and threat models. We further show that Out-of-Distribution attacks can outperform In-Distribution attacks on classifiers defended using the adversarial training defense, exposing a new attack vector for these defenses.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sitawarin_darts_2018</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Bhagoji, Arjun Nitin and Mosenia, Arsalan and Chiang, Mung and Mittal, Prateek}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1802.06430}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1802.06430 [cs]}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{{{DARTS}}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{{DARTS}}: Deceiving Autonomous Cars with Toxic Signs}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://opg.optica.org/prj/home.cfm" rel="external nofollow noopener" target="_blank">Photon. Res.</a>
            
          </abbr>
        
      
      
        
          
          



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/photores_thumbnail.png" sizes="800px"></source>
    
    <img src="/assets/img/publication_preview/photores_thumbnail.png" class="preview z-depth-0 rounded mt-1" width="100%" height="auto" alt="photores_thumbnail.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_inversedesigned_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">Inverse-designed photonic fibers and metasurfaces for nonlinear frequency conversion (Invited)</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Weiliang
            Jin, Zin
            Lin, and Alejandro W.
            Rodriguez
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>Photon. Res.</em>,  May 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
        <a href="http://arxiv.org/abs/1711.07810" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://opg.optica.org/prj/fulltext.cfm?uri=prj-6-5-B82&amp;id=385779" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Typically, photonic waveguides designed for nonlinear frequency conversion rely on intuitive and established principles, including index guiding and bandgap engineering, and are based on simple shapes with high degrees of symmetry. We show that recently developed inverse-design techniques can be applied to discover new kinds of microstructured fibers and metasurfaces designed to achieve large nonlinear frequency-conversion efficiencies. As a proof of principle, we demonstrate complex, wavelength-scale chalcogenide glass fibers and gallium phosphide three-dimensional metasurfaces exhibiting some of the largest nonlinear conversion efficiencies predicted thus far, e.g., lowering the power requirement for third-harmonic generation by 104 and enhancing second-harmonic generation conversion efficiency by 107. Such enhancements arise because, in addition to enabling a great degree of tunability in the choice of design wavelengths, these optimization tools ensure both frequency- and phase-matching in addition to large nonlinear overlap factors.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sitawarin_inversedesigned_2018</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Jin, Weiliang and Lin, Zin and Rodriguez, Alejandro W.}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1364/PRJ.6.000B82}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Photon. Res.}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Nonlinear optics, fibers; Harmonic generation and mixing ; Nonlinear optics, devices; Computational electromagnetic methods ; Nanophotonics and photonic crystals ; Chalcogenide fibers; Harmonic generation; Light matter interactions; Microstructured fibers; Phase matching; Second harmonic generation}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{B82--B89}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OSA}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inverse-designed photonic fibers and metasurfaces for nonlinear frequency conversion (Invited)}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.osapublishing.org/prj/abstract.cfm?URI=prj-6-5-B82}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
<li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#609966">
            
              <a href="https://dls2023.ieee-security.org/" rel="external nofollow noopener" target="_blank">DLS</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="sitawarin_rogue_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">Rogue Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      <em>Chawin
            Sitawarin</em>, Arjun Nitin
            Bhagoji, Arsalan
            Mosenia, Prateek
            Mittal, and Mung
            Chiang
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv:1801.02780 [cs]</em>,  Mar 2018
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://arxiv.org/abs/1801.02780" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
        <a href="https://github.com/inspire-group/advml-traffic-sign" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We propose a new real-world attack against the computer vision based systems of autonomous vehicles (AVs). Our novel Sign Embedding attack exploits the concept of adversarial examples to modify innocuous signs and advertisements in the environment such that they are classified as the adversary’s desired traffic sign with high confidence. Our attack greatly expands the scope of the threat posed to AVs since adversaries are no longer restricted to just modifying existing traffic signs as in previous work. Our attack pipeline generates adversarial samples which are robust to the environmental conditions and noisy image transformations present in the physical world. We ensure this by including a variety of possible image transformations in the optimization problem used to generate adversarial samples. We verify the robustness of the adversarial samples by printing them out and carrying out drive-by tests simulating the conditions under which image capture would occur in a real-world scenario. We experimented with physical attack samples for different distances, lighting conditions and camera angles. In addition, extensive evaluations were carried out in the virtual setting for a variety of image transformations. The adversarial samples generated using our method have adversarial success rates in excess of 95% in the physical as well as virtual settings.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">sitawarin_rogue_2018</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sitawarin, Chawin and Bhagoji, Arjun Nitin and Mosenia, Arsalan and Mittal, Prateek and Chiang, Mung}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{All rights reserved}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1801.02780}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1801.02780 [cs]}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">mar</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Rogue Signs}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rogue Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2017</h2>
<ol class="bibliography"><li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#b31b1b">
            
              <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Preprint</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="martinez_grand_2017" class="col-sm-8">
    <!-- Title -->
    <div class="title">Beyond Grand Theft Auto v for Training, Testing and Enhancing Deep Learning in Self Driving Cars</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Mark Anthony
            Martinez, <em>Chawin
            Sitawarin</em>, Kevin
            Finch, Lennart
            Meincke, Alexander
            Yablonski, and Alain
            Kornhauser
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>arXiv:1712.01397 [cs]</em>,  Dec 2017
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="https://dl.acm.org/citation.cfm?id=3278515" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>As an initial assessment, over 480,000 labeled virtual images of normal highway driving were readily generated in Grand Theft Auto V’s virtual environment. Using these images, a CNN was trained to detect following distance to cars/objects ahead, lane markings, and driving angle (angular heading relative to lane centerline): all variables necessary for basic autonomous driving. Encouraging results were obtained when tested on over 50,000 labeled virtual images from substantially different GTA-V driving environments. This initial assessment begins to define both the range and scope of the labeled images needed for training as well as the range and scope of labeled images needed for testing the definition of boundaries and limitations of trained networks. It is the efficacy and flexibility of a "GTA-V"-like virtual environment that is expected to provide an efficient well-defined foundation for the training and testing of Convolutional Neural Networks for safe driving. Additionally, described is the Princeton Virtual Environment (PVE) for the training, testing and enhancement of safe driving AI, which is being developed using the video-game engine Unity. PVE is being developed to recreate rare but critical corner cases that can be used in re-training and enhancing machine learning models and understanding the limitations of current self driving models. The Florida Tesla crash is being used as an initial reference.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">martinez_grand_2017</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Martinez, Mark Anthony and Sitawarin, Chawin and Finch, Kevin and Meincke, Lennart and Yablonski, Alexander and Kornhauser, Alain}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{1712.01397}</span><span class="p">,</span>
  <span class="na">eprinttype</span> <span class="p">=</span> <span class="s">{arxiv}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:1712.01397 [cs]}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Beyond Grand Theft Auto v for Training, Testing and Enhancing Deep Learning in Self Driving Cars}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2016</h2>
<ol class="bibliography"><li>
<div class="row mt-4 mb-5">
  
    <div class="col col-sm-2 abbr">
          <abbr class="badge rounded w-100" style="background-color:#1f5e96">
            
              <a href="https://www.cleoconference.org/" rel="external nofollow noopener" target="_blank">CLEO</a>
            
          </abbr>
        
      
      
    </div>
  

  <!-- Entry bib key -->
  <div id="lin_inversedesigned_2016" class="col-sm-8">
    <!-- Title -->
    <div class="title">Inverse-Designed Nonlinear Nanophotonic Structures: Enhanced Frequency Conversion at the Nano Scale</div>
    <!-- Author -->
    <div class="author mt-2">
      

      
      Zin
            Lin, <em>Chawin
            Sitawarin</em>, Marko
            Loncar, and Alejandro W.
            Rodriguez
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2016 Conference on Lasers and Electro-Optics, CLEO 2016</em>,  Dec 2016
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
      
      
      
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
      
      
      
        
          <a href="http://ieeexplore.ieee.org/document/7788596/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
        
      
      
      
      
      
      
      
      
    </div>
    
      
      

      
      

      
      

      
      
      
    

    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>\textcopyright 2016 OSA. We describe a large-scale computational approach based on topology optimization that enables automatic discovery of novel nonlinear photonic structures. As examples, we design complex cavity and fiber geometries that can achieve high-efficiency nonlinear frequency conversion.</p>
      </div>
    

    
      <!-- Hidden bibtex block -->
      <div class="bibtex hidden">
        <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lin_inversedesigned_2016</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Zin and Sitawarin, Chawin and Loncar, Marko and Rodriguez, Alejandro W.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 {{Conference}} on {{Lasers}} and {{Electro}}-{{Optics}}, {{CLEO}} 2016}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-943580-11-8}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inverse-Designed Nonlinear Nanophotonic Structures: Enhanced Frequency Conversion at the Nano Scale}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span>
<span class="p">}</span></code></pre></figure>
      </div>
    

    
  </div>
</div>
</li></ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    


    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    
  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip();
    });
  </script>


  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script>
<script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script>



    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script>
    <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script>
  


    

    
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      window.dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', '');
  </script>


  <!-- Cronitor RUM -->
  <script async src="https://rum.cronitor.io/script.js"></script>
  <script>
    window.cronitor =
      window.cronitor ||
      function () {
        (window.cronitor.q = window.cronitor.q || []).push(arguments);
      };
    cronitor('config', { clientKey: '' });
  </script>


  <script defer src="https://api.pirsch.io/pa.js" id="pianjs" data-code=""></script>


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    
  <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script>
  <script>
    addBackToTop();
  </script>


    

  </body>
</html>
